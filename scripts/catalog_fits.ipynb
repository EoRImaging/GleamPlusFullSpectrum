{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyradiosky import SkyModel, utils\n",
    "import numpy as np\n",
    "from astropy.table import Table, setdiff\n",
    "from astropy.utils.diff import report_diff_values\n",
    "from astropy.io import fits\n",
    "from operator import itemgetter\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import plotly.graph_objects as go\n",
    "import psutil\n",
    "import erfa\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from fpdf import FPDF\n",
    "\n",
    "sm = SkyModel()\n",
    "\n",
    "#gleam_catalog = sm.from_gleam_catalog(\"/Users/Kiana1/uwradcos/gleam.vot\", spectral_type = \"subband\", with_error = True)\n",
    "#gleam_spectral_index = sm.from_gleam_catalog(\"/Users/Kiana1/uwradcos/gleam.vot\", spectral_type = \"spectral_index\", with_error = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_linear_fit(freqs, fit_data, stokes_error, dec, catalog, detect_outlier = False):\n",
    "    \"\"\"\n",
    "    This is the fit modeling function. It compute combined error, fits the log of source data to a linear\n",
    "    polynomial, and calculates a chi2 residual. It can also perform an outlier analysis, to see if any\n",
    "    significant outliers exist for a given source.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    freqs : ndarray\n",
    "        Frequencies available for the source.\n",
    "    fit_data : ndarray\n",
    "        Fluxes for available frequencies.\n",
    "    stokes_error : ndarray\n",
    "        The error for the source in the catalog.\n",
    "    dec : ndarray\n",
    "        Declination of the source. Total error is a combination of the stokes_error and an\n",
    "        error calculated from the declination of the source.\n",
    "    detect_outlier : bool\n",
    "        Will test the source data to determine if there is a significant outlier that affects the\n",
    "        fit. When 'detect_outlier'=True, fits on all points, then determines the greatest outlier from\n",
    "        the first fit by detetermining the largest residual from the first fit. Fits again without this\n",
    "        outlier. If the reduced chi2 is improved by more than 2.6x, the outlier is removed and the fit\n",
    "        without this point is used.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    coeffs : ndarray\n",
    "        Coefficients of the linear fit of data. A pair of numbers, [b, m] corresponding to the equation\n",
    "        y=b+mx\n",
    "    chi2_residual : float\n",
    "        The reduced chi^2 value for the fit. When 'detect_outlier'=True, the chi2_residual of the better fit\n",
    "    fitted_data : ndarray\n",
    "        Modeled fluxes at only the frequencies provided in 'freqs'\n",
    "    all_freqs_fitted_data : ndarray\n",
    "        Modeled fluxes at all GLEAM frequencies\n",
    "    fitted_freqs : ndarray\n",
    "        Only frequencies that are used in the fit\n",
    "    fit_data_selected : ndarray\n",
    "        Fluxes corresponding to the 'fitted_freqs' frequencies. Sometimes there are NaN values in\n",
    "        only one of fluxes or errors, so this removes those datapoints. This usually happens if\n",
    "        the flux at a frequency was negative, and has been turned into a NaN.\n",
    "    original_parameters : ndarray\n",
    "        If an outlier is determined and the outlier-removed fit is superior to the full fit, an array containing\n",
    "        the fit parameters that included the outlier. If there is no outlier, 'original_parameters'=NaN\n",
    "    low_fit : float\n",
    "        The projected flux at 50 MHz. This is useful if you want to see how the extrapolation of the fit\n",
    "        performs outside GLEAM's frequency range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate sky coordinate-based portion of error\n",
    "    if (dec >= 18.5) or (dec <= -72):\n",
    "        loc_error = fit_data * .03\n",
    "    else:\n",
    "        loc_error = fit_data * .02\n",
    "    \n",
    "    # Compute total error and weight for the polyfit\n",
    "    total_error = np.sqrt(loc_error**2 + stokes_error**2)\n",
    "    \n",
    "    #with warnings.catch_warnings():\n",
    "            \n",
    "        # Ignore warnings caused by some sources having 1 or 0 data points\n",
    "    #    warnings.filterwarnings(action = \"ignore\", message=\"Degrees of freedom <= 0 for slice.\")\n",
    "    #    warnings.filterwarnings(action = \"ignore\", message=\"divide by zero encountered in log10\")\n",
    "            \n",
    "    weight = np.log10(1 / total_error)\n",
    "    \n",
    "    # Convert data into log scale for polyfit\n",
    "    fit_data_log = np.log10(fit_data)\n",
    "    freqs_log = np.log10(freqs)\n",
    "    all_freqs_log = np.log10(catalog.freq_array.value)\n",
    "    \n",
    "    # Subset to freqs with no nans in vals or errors and do polyfit on only those freqs\n",
    "    idx = np.isfinite(freqs_log) & np.isfinite(fit_data_log) & np.isfinite(weight)\n",
    "    \n",
    "    # coeffs is a pair of numbers (b, m) corresponding to the equation y=b+mx\n",
    "    coeffs = poly.polyfit(freqs_log[idx], fit_data_log[idx], w = weight[idx], deg=1)\n",
    "    \n",
    "    # Use coeffs to generate modeled vals at only freqs that were used to make coeffs\n",
    "    fit_log = poly.polyval(freqs_log[idx], coeffs)\n",
    "    fitted_data = 10**fit_log\n",
    "    \n",
    "    # use coefficients to generate modeled vals at all 20 freqs\n",
    "    full_fit_log = poly.polyval(all_freqs_log, coeffs)\n",
    "    all_freqs_fitted_data = 10**full_fit_log\n",
    "    \n",
    "    # generate modeled val at 50 MHz\n",
    "    low_fit_log = poly.polyval(np.log10(50000000), coeffs)\n",
    "    low_fit = 10**low_fit_log\n",
    "    \n",
    "    #compute reduced chi2 value\n",
    "    variance = total_error[idx]**2\n",
    "    residual = fit_data[idx] - fitted_data\n",
    "    chi2 = sum((residual**2) / variance)\n",
    "    chi2_residual = chi2 / (len(freqs[idx]) - 2)\n",
    "    \n",
    "    fitted_freqs = freqs[idx]\n",
    "    fit_data_selected = fit_data[idx]\n",
    "    \n",
    "    original_parameters = np.array([[float(\"NaN\")]])\n",
    "    \n",
    "    # Outlier detection reruns fit without greatest outlier\n",
    "    if detect_outlier == True:\n",
    "        idx_outlier = np.argmax(abs(residual))\n",
    "        \n",
    "        # create datasets with outlier removed\n",
    "        log_data_ol = np.delete(fit_data_log[idx], idx_outlier)\n",
    "        log_freq_ol = np.delete(freqs_log[idx], idx_outlier)\n",
    "        weight_ol = np.delete(weight[idx], idx_outlier)\n",
    "        \n",
    "        #fit without the outlier\n",
    "        coeffs_ol = poly.polyfit(log_freq_ol, log_data_ol, w = weight_ol, deg=1)\n",
    "        \n",
    "        fit_log_ol = poly.polyval(log_freq_ol, coeffs_ol)\n",
    "        fitted_data_ol = 10**fit_log_ol\n",
    "        full_fit_log_ol = poly.polyval(all_freqs_log, coeffs_ol)\n",
    "        all_freqs_fitted_data_ol = 10**full_fit_log_ol\n",
    "        \n",
    "        # compute chi2 using this new fit\n",
    "        variance_ol = np.delete(total_error[idx], idx_outlier)**2\n",
    "        residual_ol = np.delete(fit_data[idx], idx_outlier) - np.delete(fitted_data, idx_outlier)\n",
    "        \n",
    "        chi2_ol = sum((residual_ol**2) / variance_ol)\n",
    "        chi2_residual_ol = chi2_ol / (len(np.delete(freqs[idx], idx_outlier)) - 2)\n",
    "        \n",
    "\n",
    "        # see if fit has improved\n",
    "        if chi2_residual_ol < chi2_residual / 2.6:\n",
    "            \n",
    "            original_parameters = np.array([coeffs, chi2_residual, fitted_data, all_freqs_fitted_data, fitted_freqs, fit_data_selected], dtype=object)\n",
    "            \n",
    "            #reassign values with outlier removed version of fit\n",
    "            chi2_residual = chi2_residual_ol\n",
    "            coeffs = coeffs_ol\n",
    "            fitted_data = fitted_data_ol\n",
    "            all_freqs_fitted_data = all_freqs_fitted_data_ol\n",
    "            fitted_freqs = np.delete(freqs[idx], idx_outlier)\n",
    "            fit_data_selected = np.delete(fit_data[idx], idx_outlier)\n",
    "\n",
    "    return(coeffs, chi2_residual, fitted_data, all_freqs_fitted_data, fitted_freqs, fit_data_selected, original_parameters, low_fit)\n",
    "\n",
    "def chi2_one_percent(list_of_freqs):\n",
    "    \"\"\"\n",
    "    Checks how many frequencies are present and selects the appropriate chi2 value corresponding\n",
    "    to a p=.01 value. The chi2 values correspond to the degrees of freedom, which is:\n",
    "    total number of frequencies - number of fitting coefficients\n",
    "    For gleam, there are 2 fitting coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_freqs : \n",
    "        The frequencies to be checked against a chi2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    chi2_val : \n",
    "        The chi2 value that corresponds to the number of frequencies.\n",
    "    \"\"\"\n",
    "    # Reduced chi2 values corresponding to the number of frequencies.\n",
    "    chi2_dict = {20:1.934, 19:1.965, 18:2.0, 17:2.039, 16:2.082, 15:2.130, 14:2.185, 13:248, 12:2.321,\n",
    "                 11:2.407, 10:2.511, 9:2.639, 8:2.802, 7:3.017, 6:3.319, 5:3.780, 4:4.605, 3:6.635}\n",
    "    \n",
    "    tot_freqs = len(list_of_freqs)\n",
    "    chi2_val = chi2_dict[tot_freqs]\n",
    "    \n",
    "    return(chi2_val)\n",
    "\n",
    "def low_freq_fit(catalog_loc, flux_threshold = 1, sources_list = None, save_csv = None):\n",
    "    \"\"\"\n",
    "    This function performs a multi-layered fit of the GLEAM catalog sources with a preference for low frequencies.\n",
    "    A fit is always based on at least 4 data points.\n",
    "    For a given source;\n",
    "        1. Replace any negative fluxes with NaN's\n",
    "        2. Run 'log_linear_fit' function on source data, checking for significant outliers.\n",
    "        3. If the source is >1 Jy at 150 MHz, and if the reduced chi2 of the fit is >= 1.93, run 'log_linear_fit'\n",
    "        again, on only the bottom half of available frequencies for the source. Sources dimmer than 1 Jy do not\n",
    "        show much fit improvement by reducing the number of frequencies, because their scatter is high.\n",
    "        4. If the reduced chi2 of the fit of the bottom half of frequencies is still >= 1.93, AND there are at\n",
    "        least 8 frequencies in the bottom half of frequencies, run 'log_linear_fit' a third time on only the\n",
    "        bottom quarter of frequencies for the source.\n",
    "        5. If the reduced chi2 is still >= 1.93, the fit with the lowest chi2 is selected as the best fit.\n",
    "        6. Parameters and data for the best/final fit are put into a dict, which also includes a keyword whose\n",
    "        value is the relevant parameters of any previous fits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_loc : str\n",
    "        The full file location of the gleam catalog.\n",
    "    flux_threshold : float\n",
    "        The flux threshold at 150 MHz above which multiple fits are allowed. Dim sources tend to have a lot\n",
    "        of scatter across frequencies and reducing the number of datapoints by performing multiple fits\n",
    "        usually results in a fit that doesn't resemble the overall data trend for these sources.\n",
    "    sources_list : list\n",
    "        A list of specific sources. This is most useful if you want to look at sources you've noted using the\n",
    "        plotFits function, but could be used on any list of sources.\n",
    "    save_csv : str\n",
    "        The path to the folder to save a csv of the fit data. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    source_dict : dict\n",
    "        A dictionary containing dicts of the data and parameters for the best fit for each source.\n",
    "        For a given source, the dict contains:\n",
    "            ra : ndarray\n",
    "                The Right Ascension of the source\n",
    "            dec : ndarray\n",
    "                The declination of the source.\n",
    "            coefficients : ndarray\n",
    "                The coefficients of the best fit, [b, m] corresponding to y=b+mx, the linear fit of logged\n",
    "                source data.\n",
    "            chi2_residual : float\n",
    "                The reduced chi2 value of the best fit of the source.\n",
    "            prev_fit_data : list\n",
    "                Data from previous fits that were not the best fit. If a source did not have a specific fit,\n",
    "                the value is NaN. For example if there were only a full and half fit, values corresponding\n",
    "                to the quarter fit will be NaN.\n",
    "                This contains, in order:\n",
    "                1. Data for all frequencies 'all_freqs_fitted_data' from full fit\n",
    "                2. Data for all frequencies from half fit\n",
    "                3. Data for all frequencies from quarter fit\n",
    "                4. Chi2 residual 'chi2_residual' from full fit\n",
    "                5. Chi2 residual from half fit\n",
    "                6. Chi2 residual from quarter fit\n",
    "                7. Data from only provided freqs, from the variable 'fitted_data', from full fit\n",
    "                8. Data from only provided freqs used in half fit\n",
    "                9. Data from only provided freqs used in quarter fit\n",
    "                10. Coefficients from the initial full fit, from the variable 'coeffs'.\n",
    "        \n",
    "            fitted_data : ndarray\n",
    "                Modeled fluxes at only the frequencies provided in the best fit of the source. If the half fit\n",
    "                is the best, this will just be the data corresponding to the bottom half of frequencies, etc.\n",
    "            all_freqs_fitted_data : ndarray\n",
    "                Modeled fluxes at all 20 GLEAM frequencies, even frequencies with no data from the catalog.\n",
    "            freqs : ndarray\n",
    "                All 20 frequencies in GLEAM\n",
    "            freqs_used_for_fit : ndarray\n",
    "                Only frequencies used in the best fit of the source. If the half fit is the best, this is just the\n",
    "                bottom half of frequencies, etc. Corresponds to the 'fitted_freqs' output from 'log_linear_fit' \n",
    "            data_used_for_fit : ndarray\n",
    "                Original GLEAM fluxes corresponding to the frequencies used to generate the fit of the source.\n",
    "            pre_outlier_removal_output : ndarray\n",
    "                Contains data from the 'original_parameters' output from 'log_linear_fit'. These are all the data, \n",
    "                coefficients, etc from a fit where outliers were checked for and if necessary removed, before\n",
    "                removal. In other words this is the fit if the outlier is present.\n",
    "            variance : float\n",
    "                Variance is a measure of the scatter of the original flux data. Flux data is normalized,\n",
    "                and then the difference between each pair of adjacent points is calculated. The variance describes\n",
    "                overall how closely aligned adjacent datapoints are. A low variance indicates a low scatter in the\n",
    "                source across frequencies, and datapoints that follow an overall trend. High variance indicates\n",
    "                that there is a lot of intrinsic data scatter. This can be useful in looking at overall goodness of\n",
    "                fit, since we'd expect sources with a lot of intrinsic scatter and therefore a high variance to\n",
    "                have mediocre fits due to data quality issues, rather than the fit itself being at fault.\n",
    "                Conversely, a poor fit on a source with low variance would be something to be concerned about.\n",
    "            50_mhz_extrapolation : list\n",
    "                 - Projected flux at 50 MHz for the best of all fits performed.\n",
    "                 - Projected flux at 50 MHz for the initial fit on all available frequencies.\n",
    "    Output\n",
    "    ------\n",
    "    If 'save_csv' is not None, a csv of 'source_dict' will be saved in the location specified with the\n",
    "    'save_csv' kwarg.\n",
    "    \n",
    "    \"\"\"\n",
    "    gleam_catalog = sm.from_gleam_catalog(catalog_loc, spectral_type = \"subband\", with_error = True)\n",
    "    \n",
    "    #Initialize arrays used in function\n",
    "    source_dict = {}\n",
    "    bad_chi2 = []\n",
    "    problem_objs = []\n",
    "    \n",
    "    # If a list of sources is given, create array of those sources. Otherwise use an ordered list of length\n",
    "    # equal to the total Ncomponents in gleam.\n",
    "    if sources_list == None:\n",
    "        r_sources = np.arange(gleam_catalog.Ncomponents)\n",
    "    else:\n",
    "        r_sources = np.asarray(sources_list)\n",
    "        \n",
    "    # Fit each source.\n",
    "    for source in r_sources:\n",
    "        \n",
    "        fit_data = gleam_catalog.stokes.value[0,:,source]\n",
    "        dec = gleam_catalog.dec.value[source]\n",
    "        freqs = gleam_catalog.freq_array.value\n",
    "        stokes_error = gleam_catalog.stokes_error.value[0,:,source]\n",
    "\n",
    "        # Normalize data to calc variance\n",
    "        mean_adj_data = (fit_data - np.nanmean(fit_data)) / np.nanmean(fit_data) \n",
    "        \n",
    "        # Calculate variance between fluxes for final dict\n",
    "        diff = np.diff(mean_adj_data)\n",
    "        source_variance = np.nanvar(diff)\n",
    "\n",
    "        #Initialize arrays for half and quarter fits\n",
    "        out2 = np.array([[float(\"NaN\")], [float(\"NaN\")], [float(\"NaN\")], [float(\"NaN\")]])\n",
    "        out3 = np.array([[float(\"NaN\")], [float(\"NaN\")], [float(\"NaN\")], [float(\"NaN\")]])\n",
    "\n",
    "        # Find sources that have missing values in only one of error and vals\n",
    "        source_probs = []\n",
    "        for i in range(len(fit_data)):\n",
    "            if np.isnan(fit_data[i]):\n",
    "                if ~np.isnan(stokes_error[i]) and not source_probs:\n",
    "                    source_probs.append([fit_data, stokes_error])\n",
    "            else:\n",
    "                if np.isnan(stokes_error[i]) and not source_probs:\n",
    "                    source_probs.append([fit_data, stokes_error])\n",
    "\n",
    "        # Only include source in problems list if there WAS a problem, exclude source from all subsequent fitting\n",
    "        if source_probs:\n",
    "            problem_objs.append([source, gleam_catalog.ra.value[source], gleam_catalog.dec.value[source], source_probs])\n",
    "            continue\n",
    "\n",
    "        # Eliminate negative fluxes by turning into nans before fitting\n",
    "        fit_data[fit_data < 0] = np.nan\n",
    "        indices = np.argwhere(~np.isnan(fit_data)).flatten()\n",
    "\n",
    "        # Skip sources with zero fluxes (can happen if there were a small number of negative fluxes for the source)\n",
    "        if np.all(np.isnan(fit_data)):\n",
    "            continue\n",
    "\n",
    "        # Perform full fit using all freqs available for source, testing for outliers\n",
    "        out1 = log_linear_fit(freqs, fit_data, stokes_error, dec, catalog=gleam_catalog, detect_outlier = True)\n",
    "        \n",
    "        #Transfer output to 'out', which is the final output variable, to save results from this fit in out1\n",
    "        out = out1\n",
    "\n",
    "        # if chi2_residual is >=1.93 and brighter than 1Jy at 150MHz, fit again with fewer freqs\n",
    "        if out[1] >= chi2_one_percent(out[4]):\n",
    "            if fit_data[9]>=flux_threshold:\n",
    "\n",
    "                # Fit with bottom half of freqs\n",
    "                if len(fit_data[indices]) >= 8:\n",
    "                    half_freqs = freqs[indices[:int(len(indices) / 2)]]\n",
    "                    fit_data_half = fit_data[indices[:int(len(indices) / 2)]]\n",
    "                    error_half = stokes_error[indices[:int(len(indices) / 2)]]\n",
    "\n",
    "                    out2 = log_linear_fit(half_freqs, fit_data_half, error_half, dec, catalog=gleam_catalog)\n",
    "                    out = out2\n",
    "\n",
    "                    # if half fit has poor chi2, fit with bottom 1/4 freqs\n",
    "                    if out[1] >= chi2_one_percent(out[4]):\n",
    "                        \n",
    "                        # If there are >=16 total non-nan frequencies, fit on bottom 1/4\n",
    "                        if len(half_freqs) >= 8:\n",
    "                            qt_freqs = half_freqs[:int(len(half_freqs) / 2)]\n",
    "                            fit_data_qt = fit_data_half[:int(len(half_freqs) / 2)]\n",
    "                            error_qt = error_half[:int(len(half_freqs) / 2)]\n",
    "\n",
    "                            out3 = log_linear_fit(qt_freqs, fit_data_qt, error_qt, dec, catalog=gleam_catalog)\n",
    "                            out = out3\n",
    "\n",
    "                        # If there are <16 total non-nan frequencies, fit on bottom 4 frequencies\n",
    "                        else:\n",
    "                            bottom_freqs = freqs[indices[:4]]\n",
    "                            fit_data_bottom = fit_data[indices[:4]]\n",
    "                            error_bottom = stokes_error[indices[:4]]\n",
    "\n",
    "                            out3 = log_linear_fit(bottom_freqs, fit_data_bottom, error_bottom, dec, catalog=gleam_catalog)  \n",
    "                            out = out3\n",
    "\n",
    "                else:\n",
    "                    # If bottom half of freqs is <8, run \"half\" fit on bottom 4 freqs, and do not attempt 3rd fit\n",
    "                    bottom_freqs = freqs[indices[:4]]\n",
    "                    fit_data_bottom = fit_data[indices[:4]]\n",
    "                    error_bottom = stokes_error[indices[:4]]\n",
    "\n",
    "                    out2 = log_linear_fit(bottom_freqs, fit_data_bottom, error_bottom, dec, catalog=gleam_catalog)\n",
    "                    out = out2\n",
    "\n",
    "\n",
    "        # if chi2_residual is still >=1.93 after all iterations, keep fit with lowest chi2 as the best fit\n",
    "        if out[1] >= chi2_one_percent(out[4]):\n",
    "            bad_chi2.append([source, out1[3], out2[3], out3[3], out1[1], out2[1], out3[1]])\n",
    "\n",
    "            # select best of 3 fit options by lowest chi2 val and use as final fit\n",
    "            # This preferences fits with more freqs as well\n",
    "            prev_rounds = {\"out1\": out1[1], \"out2\": out2[1], \"out3\": out3[1]}\n",
    "            best_fit = min(prev_rounds, key=prev_rounds.get)\n",
    "            out = eval(best_fit)\n",
    "\n",
    "        # Create dict with final vals\n",
    "        source_vars = {\n",
    "            \"ra\": gleam_catalog.ra.value[source],\n",
    "            \"dec\": dec,\n",
    "            \"coefficients\": out[0],\n",
    "            \"chi2_residual\": out[1],\n",
    "            \"prev_fit_data\": [out1[3], out2[3], out3[3], out1[1], out2[1], out3[1], out1[2], out2[2], out3[2], \n",
    "                              out1[0]],\n",
    "            \"fitted_data\": out[2],\n",
    "            \"all_freqs_fitted_data\": out[3],\n",
    "            \"freqs\": freqs,\n",
    "            \"freqs_used_for_fit\": out[4],\n",
    "            \"data_used_for_fit\": out[5],\n",
    "            \"pre_outlier_removal_output\": out[6],\n",
    "            \"variance\": source_variance,\n",
    "            \"50_mhz_extrapolation\": [out[7], out1[7]]\n",
    "        }\n",
    "        \n",
    "        # source_dict is a dict of dicts\n",
    "        source_dict[source] = source_vars\n",
    "    \n",
    "    # Create csv of results of fitting\n",
    "    if save_csv is not None:\n",
    "        fit_output = pd.DataFrame(source_dict).T\n",
    "        fit_output.to_csv(save_csv + \"gleam_catalog_fits_\" + str(flux_threshold) + \"jy.csv\")\n",
    "        \n",
    "    return(source_dict, problem_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_ind_dist(source_dict, plot_type='diff', flux_threshold = 1, save_loc = None):\n",
    "    \"\"\"\n",
    "    Creates a 2D histogram of the distribution of source brightness at 150 MHz vs spectral index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    source_dict : dict\n",
    "        The output from 'low_freq_fit', a dict of data and parameters describing the fits of sources\n",
    "    plot_type : str\n",
    "        Which fit iteration spectral index to plot. Must be one of:\n",
    "        - \"diff\" (Difference between initial and final fit spectral indices)\n",
    "        - \"first\" (Spectral index of initial, all-frequency fits)\n",
    "        - \"final\" (Spectral index of final, best fits)\n",
    "    flux_threshold : float\n",
    "        Flux threshold used in 'low_freq_fit'. Should be identical to that threshold. Default is 1Jy.\n",
    "    save_loc : str\n",
    "        Path of folder to save plots in.\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "    Plot of brightness distribution on x axis and spectral index (or difference between spectral indices)\n",
    "    on y axis.\n",
    "    \n",
    "    If plot_type = \"diff\", there will be no sources below the flux threshold specified in 'low_freq_fit',\n",
    "    because only 1 fit is done at sources dimmer than that threshold.\n",
    "    \n",
    "    If \"save_loc\" is specified, the plot will be saved in that location as:\n",
    "    \"path.../flux150_vs_spectral_index_hist_[plot_type].png\"\n",
    "    \n",
    "    \"\"\"\n",
    "    midband = []\n",
    "    diff = []\n",
    "    first = []\n",
    "    last = []\n",
    "\n",
    "    for i in source_dict:\n",
    "        if i in source_dict:\n",
    "            if plot_type=='diff':\n",
    "                \n",
    "                # Ignore sources with only one fit\n",
    "                if np.isnan(source_dict[i]['prev_fit_data'][1][0]):\n",
    "                    continue\n",
    "                \n",
    "                # Take difference between full fit and final fit\n",
    "                else:\n",
    "                    midband.append(source_dict[i]['prev_fit_data'][0][9])\n",
    "                    first_ind = source_dict[i]['prev_fit_data'][9][1]\n",
    "                    last_ind = source_dict[i]['coefficients'][1]\n",
    "                    diff.append(last_ind - first_ind)\n",
    "            \n",
    "            # Get data from all-frequency full fits\n",
    "            elif plot_type=='first':\n",
    "                midband.append(source_dict[i]['prev_fit_data'][0][9])\n",
    "                first.append(source_dict[i]['prev_fit_data'][9][1])\n",
    "                \n",
    "            # Get data from whatever the best fit was\n",
    "            elif plot_type=='last':\n",
    "                midband.append(source_dict[i]['prev_fit_data'][0][9])\n",
    "                last.append(source_dict[i]['coefficients'][1])\n",
    "    \n",
    "    # Plot the given plot type\n",
    "    if plot_type=='diff':\n",
    "        plt.hist2d(midband, diff, bins = 100, norm=LogNorm(), range = [[0,5],[-16,16]])\n",
    "        plt.ylabel(\"Spectral index diff, [last - first]\")\n",
    "        plt.title(\"Distribution of source flux vs spectral index, [final - first] fit, multifit threshold: \"\n",
    "                  + str(flux_threshold) + \" Jy\")\n",
    "\n",
    "    elif plot_type=='first':\n",
    "        plt.hist2d(midband, first, bins = 100, norm=LogNorm(), range = [[0,5],[-10,10]])\n",
    "        plt.ylabel(\"Spectral index, first fit\")\n",
    "        plt.title(\"Distribution of source flux vs spectral index, \" + plot_type +\n",
    "              \" fit, multifit threshold: \" + str(flux_threshold) + \" Jy\")\n",
    "        \n",
    "    elif plot_type=='last':\n",
    "        plt.hist2d(midband, last, bins = 100, norm=LogNorm(), range = [[0,5],[-10,10]])\n",
    "        plt.ylabel(\"Spectral index, final fit\")\n",
    "        plt.title(\"Distribution of source flux vs spectral index, \" + plot_type +\n",
    "              \" fit, multifit threshold: \" + str(flux_threshold) + \" Jy\")\n",
    "        \n",
    "    plt.xlabel(\"150 Mhz flux\")\n",
    "\n",
    "    if save_loc is not None:\n",
    "        filepath = save_loc + \"flux150_vs_spectral_index_hist_\" + plot_type + \"_\" + str(flux_threshold) + \"jy.png\"\n",
    "        plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_ind_dist(source_dict = source_fits, plot_type='first', save_loc = '/Users/Kiana1/uwradcos/plots/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def grouper(n, iterable):\n",
    "    \"\"\"\n",
    "    Creates groups of n over iterable = range(N)\n",
    "    \"\"\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return ([e for e in t if e is not None] for t in itertools.zip_longest(*args))\n",
    "\n",
    "\n",
    "def compile_plots(plot_locs, length):\n",
    "    \"\"\"\n",
    "    Compiles all plots with a certain naming convention into a single pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    plot_locs : str\n",
    "        Full path name to jpgs, excluding the number on the end of the jpg indicating the page number,\n",
    "        and the .jpg extension itself. \n",
    "    length : int\n",
    "        Number of pages to combine+1. So if 20 pages, length=21.\n",
    "    Output\n",
    "    ------\n",
    "    PDF of all jpg pages combined into one file\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    pdf = FPDF()\n",
    "    wi,h = 0,0\n",
    "\n",
    "    # Combining output files \n",
    "    for i in range(0, length):\n",
    "        fname = plot_locs + \"%.1d.jpg\" % i\n",
    "        if os.path.exists(fname):\n",
    "            if i == 0:\n",
    "                cover = Image.open(fname)\n",
    "                wi,h = cover.size\n",
    "                pdf = FPDF(unit = \"pt\", format = [wi,h])\n",
    "            image = fname\n",
    "            pdf.add_page()\n",
    "            pdf.image(image,0,0,wi,h)\n",
    "        else:\n",
    "            print(\"File not found:\", fname)\n",
    "        print(\"processed page %d\" % i, fname)\n",
    "    pdf.output(plot_locs + \"_all.pdf\", \"F\")\n",
    "    print(\"PDF can be found at: \" + plot_locs + \"_all.pdf\")\n",
    "    \n",
    "\n",
    "def plotFits(catalog_loc, source_dict, plot_type, order, Nsources=9, savefig=False,\n",
    "             saveloc=\"/Users/Kiana1/uwradcos/plots/compare_outlier_spectrums\"):\n",
    "    \"\"\"\n",
    "    Creates pages with 9 plots per page of original GLEAM data of whatever subset of sources you select,\n",
    "    the GLEAM catalog fits for those sources, and the fit results from the 'low_freq_fit' function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_loc : str\n",
    "        The full path location of the GLEAM catalog\n",
    "    source_dict : dict\n",
    "        The first object returned from the 'low_freq_fit' function\n",
    "    plot_type :  str\n",
    "        One of the following: 'no_dim', 'bright', 'spect_ind_outliers', 'multi', 'hera_stripe', 'all'.\n",
    "        1. 'no_dim' - sources above 100 mJ\n",
    "        2. 'bright' - sources where the all frequency fit is brighter than 1 Jy at 150 MHz\n",
    "        3. 'spect_ind_outliers' - sources with spectral indices outside of [-2, 1]\n",
    "        4. 'multi' - sources that were fitted multiple times\n",
    "        5. 'hera_stripe' - sources that fall in the HERA stripe\n",
    "        6. 'all' - all sources\n",
    "    order : str\n",
    "        One of the following: 'low_freq_compare', 'max_flux', '150mhz_flux', 'chi2', 'spect_ind'. \n",
    "        1. 'low_freq_compare' - sorts based on difference between fit extrapolation at 50 MHz of first fit and\n",
    "        final fit, from largest difference to smallest\n",
    "        2. 'max_flux' - sorts based on maximum brightness of source, bright to dim\n",
    "        3. '150mhz_flux' - sorts based on brightness of the initial all-frequency fit at 150 MHz, brightest to dimmest\n",
    "        4. 'chi2' - based on chi2 value, largest to smallest\n",
    "        5. 'spect_ind' - sorts based on the absolute value of the final spectral index of the source, largest to smallest.\n",
    "    Nsources : int\n",
    "        Number of catalog sources to plot. If less than the total number of sources that fulfill the plot_type is\n",
    "        given, plots the specified number of plots sorted by the order. For example if Nsources=30 and order=chi2,\n",
    "        it will plot the 30 largest chi2 sources from large to small.\n",
    "    savefig : bool\n",
    "        If True, will save all figures in the folder given by 'saveloc'. Many files are generated, so it's good to\n",
    "        specify a folder separate from anything else.\n",
    "    save_loc : str\n",
    "        The location for all \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    gleam_catalog = sm.from_gleam_catalog(catalog_loc, spectral_type = \"subband\", with_error = True)\n",
    "    gleam_spectral_index = sm.from_gleam_catalog(catalog_loc, spectral_type = \"spectral_index\", with_error = True)\n",
    "    \n",
    "    #get sources ordered by max power of points used for final fit\n",
    "    ordered_power = []\n",
    "    \n",
    "    # Select some subset of sources that fall under a criteria given by plot_type\n",
    "    #for k in np.arange(max(source_dict)):\n",
    "    for k in source_dict:\n",
    "        if k in source_dict:\n",
    "            \n",
    "            eval_parameter = \"r'$Sp-Ind=%.2e$' % (source_dict[source_num]['coefficients'][1])\"\n",
    "            # Create sorting variable\n",
    "            if order=='low_freq_compare':\n",
    "                sort_var = source_dict[k]['50_mhz_extrapolation'][0]\n",
    "                sort_tag = \"extrapolated flux at 50 MHz\"\n",
    "                \n",
    "                eval_parameter = \"r'$50MHz-flux=%.2e$' % (source_dict[source_num]['50_mhz_extrapolation'][0])\"\n",
    "                # Do a thing where if this is what we're looking at, it shows sthe diff instead of the\n",
    "                #spectral index in the box on the plot. Otherwise it shows the spectral index\n",
    "                \n",
    "            elif order=='max_flux':\n",
    "                sort_var = np.nanmax(source_dict[k]['data_used_for_fit'])\n",
    "                sort_tag = \"maximum flux used in fit\"\n",
    "            elif order=='150mhz_flux':\n",
    "                sort_var = source_dict[k]['prev_fit_data'][0][9]\n",
    "                sort_tag = \"modeled flux at 150 MHz\"\n",
    "            elif order=='chi2':\n",
    "                sort_var = source_dict[k]['chi2_residual']\n",
    "                sort_tag = \"final X2 value\"\n",
    "            elif order=='spect_ind':\n",
    "                sort_var = np.abs(source_dict[k]['coefficients'][1])\n",
    "                sort_tag = \"final spectral index\"\n",
    "            else:\n",
    "                raise ValueError(\"Must choose one of the following sort methods: 'low_freq_compare', 'max_flux', '150mhz_flux', 'chi2', 'spect_ind'.\")\n",
    "            \n",
    "            # Subset to sources that fulfill some critera\n",
    "            if plot_type == 'hera_stripe':\n",
    "                # Skip objs that are not in HERA stripe\n",
    "                if 105 <= source_dict[k]['ra'] <= 255:\n",
    "                    if 26 <= source_dict[k]['dec']<=34:\n",
    "                        ordered_power.append([sort_var, k])\n",
    "                        tag = \"in Hera Stripe\"\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "            elif plot_type == 'multi':   \n",
    "                # skip objs that found good fit first time\n",
    "                if np.isnan(source_dict[k]['prev_fit_data'][1][0]):\n",
    "                    continue\n",
    "                else:\n",
    "                    ordered_power.append([sort_var, k])\n",
    "                    tag = \"with multiple fits\"\n",
    "            \n",
    "            elif plot_type == 'no_dim':\n",
    "                # skip objs where max val is <100mJ\n",
    "                if np.nanmax(source_dict[k]['data_used_for_fit']) < .1:\n",
    "                    continue\n",
    "                else:\n",
    "                    ordered_power.append([sort_var, k]) \n",
    "                    tag = \"brighter than 100 mJ\"\n",
    "                \n",
    "            elif plot_type == 'bright':\n",
    "                # Skip objs where the all-freq fit is dimmer than 1Jy at 150 MHz\n",
    "                if source_dict[k]['prev_fit_data'][0][9] < 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    ordered_power.append([sort_var, k])\n",
    "                    tag = \"brighter than 1Jy at 150 MHz\"\n",
    "            \n",
    "            elif plot_type == 'spect_ind_outliers':\n",
    "                # skip objs that have close to average spectral indices\n",
    "                if -2 <= source_dict[k]['coefficients'][1] <= 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    ordered_power.append([sort_var, k])\n",
    "                    tag = \"with Spectral Index outside of [-2,1]\"\n",
    "                    \n",
    "            elif plot_type == 'removed_outlier':\n",
    "                # Skip objects that don't have outlier info\n",
    "                if np.isnan(source_dict[k]['pre_outlier_removal_output'][0][0]):\n",
    "                    continue\n",
    "                else:\n",
    "                    ordered_power.append([sort_var, k])\n",
    "                    tag = \"with a significant outlier point\"\n",
    "            \n",
    "            elif plot_type == 'all':\n",
    "                ordered_power.append([sort_var, k])\n",
    "\n",
    "            \n",
    "    ordered_power = sorted(ordered_power, key=lambda x: x[0], reverse = True)\n",
    "    \n",
    "    if Nsources == \"all\":\n",
    "        Nsources = len(ordered_power)\n",
    "    \n",
    "    #number of pages necessary\n",
    "    grouped_sources = grouper(9, range(Nsources))\n",
    "\n",
    "    for j in list(grouped_sources):\n",
    "        #Page configuration setup\n",
    "        page = int(max(j) / 9)\n",
    "        nsources = max(j)+1 / (page + 1)\n",
    "\n",
    "        fig = plt.figure(figsize=(28,20))\n",
    "        nrows = 3\n",
    "        gs = gridspec.GridSpec(3*nrows, 3, height_ratios=np.tile([1,0.4,0.25],nrows))\n",
    "        plt.subplots_adjust(hspace=.0)\n",
    "        \n",
    "        elements = []\n",
    "        for page_idx,i in enumerate(j):\n",
    "            # 'plots' is a list of plotting code snippets, which are executed all together once all plots that exist\n",
    "            # have been determined for a source\n",
    "            plots = []\n",
    "            fit = plt.subplot(gs[int((page_idx//3)*9 + page_idx%3)])\n",
    "            source_num = ordered_power[i][1]\n",
    "            freqs = source_dict[source_num]['freqs'] / 1000000\n",
    "            \n",
    "            # plots the gleam catalog fit\n",
    "            ref_freq = gleam_spectral_index.reference_frequency.value[source_num] / 1000000\n",
    "            gleam_fit = gleam_spectral_index.stokes[0,0,source_num]*(freqs/ref_freq)**(gleam_spectral_index.spectral_index[source_num])\n",
    "            plots.append('fit.plot(freqs, gleam_fit, label=\"gleam fit\", linestyle = \":\", color = \"#7299FF\")')\n",
    "\n",
    "            # plots the raw gleam data for all frequencies with errorbars \n",
    "            raw_data = gleam_catalog.stokes.value[0,:,source_num]\n",
    "            raw_freqs = gleam_catalog.freq_array.value / 1000000\n",
    "            stokes_error = gleam_catalog.stokes_error.value[0,:,source_num]\n",
    "            plots.append('fit.errorbar(raw_freqs, raw_data, yerr = stokes_error, fmt = \"o\", label = \"raw data\", color = \"#FFC950\")')\n",
    "            \n",
    "            # Plots raw gleam data that was used in the best/final fit from 'low_freq_fit' function, with errorbars\n",
    "            raw_data_fitted = source_dict[source_num][\"data_used_for_fit\"]\n",
    "            raw_freqs_fitted = source_dict[source_num][\"freqs_used_for_fit\"] / 1000000\n",
    "            plots.append('fit.errorbar(raw_freqs_fitted, raw_data_fitted, yerr = gleam_catalog.stokes_error.value[0, 0:len(source_dict[source_num][\"freqs_used_for_fit\"]), source_num], fmt = \"o\", label = \"raw data\", color = \"#845B00\")')\n",
    "            \n",
    "            # Calculate deviation of each raw point from the mean, compute variance from this info\n",
    "            mean_adj_data = (raw_data - np.nanmean(raw_data)) / np.nanmean(raw_data)\n",
    "            diff = np.diff(mean_adj_data)\n",
    "            variance = np.nanvar(diff)\n",
    "            \n",
    "            #plot full initial fit, after outlier removal if one was present\n",
    "            full_fit_post_outlier = source_dict[source_num]['prev_fit_data'][0]\n",
    "            plots.append('fit.plot(freqs, full_fit_post_outlier, label = \"full fit\", color = \"#378A00\")')\n",
    "\n",
    "            text_content = [r'$\\mathbf{Full\\/Fit\\/\\chi^2=%.2f}$' % (source_dict[source_num]['prev_fit_data'][3]),\n",
    "                           eval(eval_parameter)]\n",
    "\n",
    "            residual = raw_data / full_fit_post_outlier\n",
    "            \n",
    "            # datapoints for the best/final fit of the source\n",
    "            post_outlier_freqs = source_dict[source_num]['freqs_used_for_fit'] / 1000000   \n",
    "            post_outlier_flux = source_dict[source_num]['fitted_data']\n",
    "            \n",
    "            # Add dataset to legend if it doesn't already exist from previous plots on the same page\n",
    "            if 'Line2D([0], [0], linestyle = \":\", color = \"#7299FF\", label=\"gleam fit\")' not in elements: elements.append('Line2D([0], [0], linestyle = \":\", color = \"#7299FF\", label=\"gleam fit\")')\n",
    "            if 'Line2D([0], [0], marker = \"o\", markersize = 15,  color = \"#FFC950\", label = \"raw data\")' not in elements: elements.append('Line2D([0], [0], marker = \"o\", markersize = 15,  color = \"#FFC950\", label = \"raw data\")')\n",
    "            if 'Line2D([0], [0], marker = \"o\", markersize = 15,  color = \"#845B00\", label = \"raw data used\")' not in elements: elements.append('Line2D([0], [0], marker = \"o\", markersize = 15,  color = \"#845B00\", label = \"raw data used\")')\n",
    "            if 'Line2D([0], [0], color = \"#378A00\", label = \"full fit\")' not in elements: elements.append('Line2D([0], [0], color = \"#378A00\", label = \"full fit\")')\n",
    "\n",
    "            # Create box with variance and spectral index\n",
    "            props_var = dict(boxstyle = \"round\", facecolor = \"wheat\", alpha = .4)\n",
    "            textstr_var = [r'$var=%.2e$' % (variance), \n",
    "                           r'$Sp-Ind=%.2e$' % (source_dict[source_num]['coefficients'][1])]\n",
    "            textstr_var = '\\n'.join((textstr_var))\n",
    "            \n",
    "            # If the only fit is first all-freq fit, and it is below the chi2 threshhold of 1.93, color plot red\n",
    "            if source_dict[source_num]['prev_fit_data'][3] >= chi2_one_percent(source_dict[source_num]['freqs_used_for_fit']) and np.isnan(np.all(source_dict[source_num]['prev_fit_data'][1])):\n",
    "                plots.append('fit.spines[\"left\"].set_color(\"red\")')\n",
    "                plots.append('fit.tick_params(axis=\"y\", colors=\"red\")')\n",
    "                plots.append('resid.spines[\"bottom\"].set_color(\"red\")')\n",
    "                plots.append('resid.spines[\"left\"].set_color(\"red\")')\n",
    "                plots.append('resid.tick_params(axis=\"y\", colors=\"red\")')\n",
    "                plots.append('resid.tick_params(axis=\"x\", colors=\"red\")')\n",
    "                plots.append('fit.patch.set_facecolor(\"#FFE4E4\")')\n",
    "                plots.append('resid.patch.set_facecolor(\"#FFE4E4\")')\n",
    "            \n",
    "            # Plot half frequencies fit if present\n",
    "            if ~np.isnan(source_dict[source_num]['prev_fit_data'][1][0]):\n",
    "                half_fit = source_dict[source_num]['prev_fit_data'][1]\n",
    "                plots.append('fit.plot(freqs, half_fit, label = \"half fit\", color = \"#BD1C6B\")')\n",
    "                \n",
    "                # Create text for chi2 box including 2 fit levels\n",
    "                text_content = [r'$Full\\/Fit\\/\\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][3]),\n",
    "                    r'$\\mathbf{Half\\/Fit\\/\\chi^2=%.2f}$' % (source_dict[source_num]['prev_fit_data'][4]),\n",
    "                               eval(eval_parameter)]\n",
    "\n",
    "                residual = abs(raw_data / half_fit)\n",
    "                if 'Line2D([0], [0], color = \"#BD1C6B\", label = \"half fit\")' not in elements: elements.append('Line2D([0], [0], color = \"#BD1C6B\", label = \"half fit\")')\n",
    "                \n",
    "                #fit.text(0.97, 0.68, textstr_var, horizontalalignment='right', verticalalignment='top', transform=fit.transAxes, bbox = props_var)\n",
    "                \n",
    "                # Plot quarter frequencies fit if present\n",
    "                if ~np.isnan(source_dict[source_num]['prev_fit_data'][2][0]):\n",
    "                    quarter_fit = source_dict[source_num]['prev_fit_data'][2]\n",
    "                    \n",
    "                    # if no good fit is found change axes color to red\n",
    "                    if source_dict[source_num]['prev_fit_data'][5] >=chi2_one_percent(source_dict[source_num]['freqs_used_for_fit']):\n",
    "                        plots.append('fit.spines[\"left\"].set_color(\"red\")')\n",
    "                        plots.append('fit.tick_params(axis=\"y\", colors=\"red\")')\n",
    "                        plots.append('resid.spines[\"bottom\"].set_color(\"red\")')\n",
    "                        plots.append('resid.spines[\"left\"].set_color(\"red\")')\n",
    "                        plots.append('resid.tick_params(axis=\"y\", colors=\"red\")')\n",
    "                        plots.append('resid.tick_params(axis=\"x\", colors=\"red\")')\n",
    "                        plots.append('fit.patch.set_facecolor(\"#FFE4E4\")')\n",
    "                        plots.append('resid.patch.set_facecolor(\"#FFE4E4\")')\n",
    "                        \n",
    "                    plots.append('fit.plot(freqs, quarter_fit, label = \"quarter fit\", color = \"#785EF0\")')\n",
    "                    \n",
    "                    # Create text for chi2 box including 3 fit levels\n",
    "                    text_content = [r'$Full\\/Fit\\/ \\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][3]),\n",
    "                        r'$Half\\/Fit\\/\\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][4]),\n",
    "                        r'$\\mathbf{Quart\\/Fit\\/\\chi^2=%.2f}$' % (source_dict[source_num]['prev_fit_data'][5]),\n",
    "                                   eval(eval_parameter)]\n",
    "\n",
    "                    residual = abs(raw_data / quarter_fit)\n",
    "                    if 'Line2D([0], [0], color = \"#785EF0\", label = \"quarter fit\")' not in elements: elements.append('Line2D([0], [0], color = \"#785EF0\", label = \"quarter fit\")')\n",
    "                    \n",
    "                    # bold lowest chi2 fit line and corresponding chi2 value in box if best fit is not quarter\n",
    "                    if source_dict[source_num]['prev_fit_data'][5] != source_dict[source_num]['chi2_residual']:\n",
    "                        best_fit = np.min([source_dict[source_num]['prev_fit_data'][3],\n",
    "                                           source_dict[source_num]['prev_fit_data'][4],\n",
    "                                           source_dict[source_num]['prev_fit_data'][5]])\n",
    "                        if best_fit == source_dict[source_num]['prev_fit_data'][3]:\n",
    "                            text_content = [r'$\\mathbf{Full\\/Fit\\/ \\chi^2=%.2f}$' % (source_dict[source_num]['prev_fit_data'][3]),\n",
    "                                            r'$Half\\/Fit\\/\\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][4]),\n",
    "                                            r'$Quart\\/Fit\\/\\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][5]),\n",
    "                                           eval(eval_parameter)]\n",
    "                            matching = [s for s in plots if \"full fit\" in s]\n",
    "                            index = matching[0].find('color')\n",
    "                            plots.append(matching[0][:index] + 'linewidth = 4, ' + matching[0][index:])\n",
    "                        \n",
    "                        if best_fit == source_dict[source_num]['prev_fit_data'][4]:\n",
    "                            text_content = [r'$Full\\/Fit\\/ \\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][3]),\n",
    "                                            r'$\\mathbf{Half\\/Fit\\/\\chi^2=%.2f}$' % (source_dict[source_num]['prev_fit_data'][4]),\n",
    "                                            r'$Quart\\/Fit\\/\\chi^2=%.2f$' % (source_dict[source_num]['prev_fit_data'][5]),\n",
    "                                           eval(eval_parameter)]\n",
    "                            matching = [s for s in plots if \"half fit\" in s]\n",
    "                            index = matching[0].find('color')\n",
    "                            plots.append(matching[0][:index] + 'linewidth = 4, ' + matching[0][index:])\n",
    "                    \n",
    "                    # Make final fit line thicker\n",
    "                    else:\n",
    "                        index = plots[-1].find('color')\n",
    "                        plots[-1] = plots[-1][:index] + 'linewidth = 4, ' + plots[-1][index:]\n",
    "                # Make full fit line thick if no half or quarter present\n",
    "                else:\n",
    "                    index = plots[-1].find('color')\n",
    "                    plots[-1] = plots[-1][:index] + 'linewidth = 4, ' + plots[-1][index:]\n",
    "                    \n",
    "            # if there was an outlier removed, include outlier lines\n",
    "            if ~np.isnan(source_dict[source_num]['pre_outlier_removal_output'][0][0]):\n",
    "\n",
    "                full_fit_pre_outlier = source_dict[source_num]['pre_outlier_removal_output'][3]\n",
    "                \n",
    "                # Fit data from before outlier was removed\n",
    "                pre_outlier_freqs = source_dict[source_num]['pre_outlier_removal_output'][4] / 1000000\n",
    "                pre_outlier_flux = source_dict[source_num]['pre_outlier_removal_output'][2]\n",
    "                \n",
    "                # Data of outlier point\n",
    "                outlier_freq = np.setdiff1d(list(pre_outlier_freqs),list(post_outlier_freqs))\n",
    "                outlier_flux = gleam_catalog.stokes.value[0,gleam_catalog.freq_array.value == outlier_freq * 1000000, source_num]\n",
    "                outlier_error = gleam_catalog.stokes_error.value[0,gleam_catalog.freq_array.value == outlier_freq * 1000000, source_num]\n",
    "\n",
    "                # Plots pre-outlier removal fits if an outlier was removed, and plots outlier point\n",
    "                plots.append('fit.plot(freqs, full_fit_pre_outlier, linestyle = \"--\", label = \"pre-outlier removal\", color = \"#002940\")')\n",
    "                plots.append('fit.errorbar(outlier_freq, outlier_flux, yerr = outlier_error, fmt=\"o\", label = \"outlier\", color = \"#9A00FF\", zorder=10)')\n",
    "\n",
    "                if 'Line2D([0], [0], color = \"#002940\", linestyle = \"--\", label = \"pre-outlier removal\")' not in elements: elements.append('Line2D([0], [0], color = \"#002940\", linestyle = \"--\", label = \"pre-outlier removal\")')\n",
    "                if 'Line2D([0], [0], marker = \"o\", markersize = 13, color = \"#9A00FF\", label = \"outlier\")' not in elements: elements.append('Line2D([0], [0], marker = \"o\", markersize = 13, color = \"#9A00FF\", label = \"outlier\")')\n",
    "\n",
    "        \n",
    "            # Specify floating box containing chi2 values \n",
    "            textstr = '\\n'.join((text_content))\n",
    "            props = dict(boxstyle = \"round\", facecolor = \"wheat\", alpha = .4)\n",
    "            \n",
    "            # Set up plot appearance for a source\n",
    "            mpl.rcParams['font.size']=14\n",
    "            fit.text(0.97, 0.95, textstr, horizontalalignment='right', verticalalignment='top', transform=fit.transAxes, bbox = props)\n",
    "            fit.set_xlim([np.min(raw_freqs)*.95, np.max(raw_freqs)*1.05])\n",
    "            fit.set_ylim([np.nanmin(gleam_catalog.stokes.value[0,:,source_num])*.9, np.nanmax(gleam_catalog.stokes.value[0,:,source_num]) * 1.2])\n",
    "            fit.set_title(\"Source \"+ str(source_num) + \", RA \" + \"{:.2f}\".format(source_dict[source_num]['ra']) + \", Dec \" + \"{:.2f}\".format(source_dict[source_num]['dec']), fontsize = 14)\n",
    "            \n",
    "            # Create bottom residuals plot\n",
    "            resid = plt.subplot(gs[int((page_idx//3)*9 + page_idx%3 + 3)])\n",
    "            \n",
    "            res = resid.scatter(freqs, residual, marker = \"P\", color = '#000000', label = \"residual\")\n",
    "            res2 = resid.axhline(y=1, linestyle = \"-\")\n",
    "            resid.set_xlim([np.min(raw_freqs)*.95, np.max(raw_freqs)*1.05])\n",
    "            if 'Line2D([0], [0], marker = \"P\", markersize = 13, color = \"#000000\", label = \"residual\")' not in elements: elements.append('Line2D([0], [0], marker = \"P\", markersize = 13, color = \"#000000\", label = \"residual\")')\n",
    "            \n",
    "            # Actually run all items in the plots list\n",
    "            for n in plots:\n",
    "                eval(n)\n",
    "        \n",
    "        # Actually run all items in elements list, which sets the appearance of the plots\n",
    "        elements_comp = []\n",
    "        for p in elements:\n",
    "            elements_comp.append(eval(p))\n",
    "        \n",
    "        # Create legend\n",
    "        fig.legend(handles = elements_comp,   \n",
    "               loc=\"center right\",            \n",
    "               borderaxespad=0.1,             \n",
    "               title=\"Dataset\",               \n",
    "               fontsize = 12,\n",
    "               title_fontsize = 14\n",
    "               )\n",
    "        \n",
    "        # Set overall info for every page\n",
    "        fig.text(0.5, 0.1, 'Frequency (MHz)', ha='center', fontsize = 16)\n",
    "        fig.text(0.08, 0.5, 'Flux (Jy)', va='center', rotation='vertical', fontsize = 16)\n",
    "        fig.text(.35, .9, \"Sources \" + tag + \", ordered by \" + sort_tag , fontsize = 16)\n",
    "        \n",
    "        # Save every page as a jpg\n",
    "        if savefig == True:\n",
    "            print(f'Saving page {page} to {saveloc + \"_\" + plot_type + \"_sort_\" + order + str(page)}.jpg')\n",
    "            savepath = saveloc + \"_\" + plot_type + \"_sort_\" + order + str(page) + '.jpg'\n",
    "            plt.savefig(savepath, format = 'jpg')\n",
    "        plt.close()\n",
    "        \n",
    "    # Combines all individual jpgs into one pdf   \n",
    "    if savefig==True:\n",
    "        pages = len(list(grouper(9, range(Nsources))))\n",
    "        print(pages)\n",
    "        savepath_gen = saveloc + \"_\" + plot_type + \"_sort_\" + order\n",
    "        compile_plots(savepath_gen, length=pages)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Selected subsets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fits, problem_objs  = low_freq_fit(catalog_loc = \"/Users/Kiana1/uwradcos/gleam.vot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_threshold_fits, problem_objs_dim = low_freq_fit(catalog_loc = \"/Users/Kiana1/uwradcos/gleam.vot\", flux_threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = no_threshold_fits, plot_type = 'multi',\n",
    "         order = 'max_flux', Nsources = 'all', savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 1230 of output_all\n",
    "source_fits, problem_objs = low_freq_fit(catalog_loc = \"/Users/Kiana1/uwradcos/gleam.vot\")\n",
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'no_dim',\n",
    "         order = 'max_flux', Nsources = 9, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 1230 of output_all\n",
    "source_fits = low_freq_fit(catalog_loc = \"/Users/Kiana1/uwradcos/gleam.vot\",\n",
    "                           sources_list = [227687, 246226, 197302, 71712, 222590, 120187, 50101, 279340, 220508])\n",
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'no_dim',\n",
    "         order = 'max_flux', Nsources = 9, savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 2097 of output_all\n",
    "source_fits = low_freq_fit(catalog_loc = \"/Users/Kiana1/uwradcos/gleam.vot\",\n",
    "                           sources_list = [155982, 152082, 53755, 29123, 185532, 92914, 31186, 52556, 119898])\n",
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'no_dim',\n",
    "         order = 'max_flux', Nsources = 9, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg 2 of output_all_spind_outliers\n",
    "source_fits = low_freq_fit(catalog_loc = \"/Users/Kiana1/uwradcos/gleam.vot\",\n",
    "                           sources_list = [64125, 292044, 290456, 173592, 243867, 10289, 274888, 260349, 45387])\n",
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'bright',\n",
    "         order = 'spect_ind', Nsources = 9, savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'spect_ind_outliers',\n",
    "         order = 'chi2', Nsources = 'all', savefig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'bright', order = 'chi2', savefig=False, Nsources=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'spect_ind_outliers', order = 'chi2', savefig=False, Nsources=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFits(catalog_loc = '/Users/Kiana1/uwradcos/gleam.vot', source_dict = source_fits, plot_type = 'spect_ind_outliers', order = 'chi2', savefig=False, Nsources=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
